{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "from data import *\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_groups = False\n",
    "automatic_adjustment = False\n",
    "use_normalized_loss = False\n",
    "lr = 0.001\n",
    "alpha = 0.2\n",
    "batch_size = 64\n",
    "weight_decay = 0\n",
    "gamma = 0.1\n",
    "n_epochs = 10\n",
    "seed = 0\n",
    "generalization_adjustment = \"0\"\n",
    "log_every = 50\n",
    "model = \"resnet50\"\n",
    "log_dir = \"./logs\"\n",
    "save_best = True\n",
    "save_last = False\n",
    "mode = 'w'\n",
    "robust = False\n",
    "robust_step_size = 0.01\n",
    "log_every = 50\n",
    "save_step = 10\n",
    "save_best = True\n",
    "save_last = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 4795 | Val data:  1199 | Test data:  1199\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "logger = Logger(os.path.join(log_dir, 'log.txt'), mode=mode)\n",
    "set_seed(seed)\n",
    "\n",
    "train_data, val_data, test_data = get_confounder_splits(\n",
    "    data_dir=\"../datasets/waterbird\", augment_data=False)\n",
    "\n",
    "loader_kwargs = {'batch_size': batch_size, 'pin_memory': False}\n",
    "train_loader = train_data.get_loader(\n",
    "    train=True, reweight_groups=reweight_groups, **loader_kwargs)\n",
    "val_loader = val_data.get_loader(\n",
    "    train=False, reweight_groups=False, ** loader_kwargs)\n",
    "test_loader = test_data.get_loader(\n",
    "    train=False, reweight_groups=False, **loader_kwargs)\n",
    "\n",
    "print(\n",
    "    f'Train data: {train_data.__len__()} | Val data:  {val_data.__len__()} | Test data:  {test_data.__len__()}')\n",
    "\n",
    "data = {}\n",
    "data['train_loader'] = train_loader\n",
    "data['val_loader'] = val_loader\n",
    "data['test_loader'] = test_loader\n",
    "data['train_data'] = train_data\n",
    "data['val_data'] = val_data\n",
    "data['test_data'] = test_data\n",
    "n_classes = train_data.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2, progress=True)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=2)\n",
    "\n",
    "logger.flush()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "train_csv_logger = CSVBatchLogger(os.path.join(\n",
    "    log_dir, 'train.csv'), train_data.n_groups, mode=mode)\n",
    "val_csv_logger = CSVBatchLogger(os.path.join(\n",
    "    log_dir, 'val.csv'), train_data.n_groups, mode=mode)\n",
    "test_csv_logger = CSVBatchLogger(os.path.join(\n",
    "    log_dir, 'test.csv'), train_data.n_groups, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0]:\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 49/75 [10:24<05:31, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average incurred loss: 0.439  \n",
      "Average sample loss: 0.439  \n",
      "Average acc: 0.799  \n",
      "[n = 2324]:\tloss = 0.269  exp loss = 0.155  adjusted loss = 0.155  adv prob = 0.250000   acc = 0.983\n",
      "[n = 132]:\tloss = 0.438  exp loss = 0.522  adjusted loss = 0.522  adv prob = 0.250000   acc = 0.939\n",
      "[n = 45]:\tloss = 1.345  exp loss = 1.477  adjusted loss = 1.477  adv prob = 0.250000   acc = 0.000\n",
      "[n = 699]:\tloss = 0.947  exp loss = 0.741  adjusted loss = 0.741  adv prob = 0.250000   acc = 0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [16:09<00:00, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average incurred loss: 0.223  \n",
      "Average sample loss: 0.223  \n",
      "Average acc: 0.940  \n",
      "[n = 1174]:\tloss = 0.111  exp loss = 0.103  adjusted loss = 0.103  adv prob = 0.250000   acc = 0.999\n",
      "[n = 52]:\tloss = 0.532  exp loss = 0.493  adjusted loss = 0.493  adv prob = 0.250000   acc = 0.750\n",
      "[n = 11]:\tloss = 1.417  exp loss = 1.471  adjusted loss = 1.471  adv prob = 0.250000   acc = 0.182\n",
      "[n = 358]:\tloss = 0.510  exp loss = 0.518  adjusted loss = 0.518  adv prob = 0.250000   acc = 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:26<00:00,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average incurred loss: 0.509  \n",
      "Average sample loss: 0.506  \n",
      "Average acc: 0.716  \n",
      "[n = 467]:\tloss = 0.096  exp loss = 0.095  adjusted loss = 0.095  adv prob = 0.250000   acc = 0.996\n",
      "[n = 466]:\tloss = 0.649  exp loss = 0.613  adjusted loss = 0.613  adv prob = 0.250000   acc = 0.577\n",
      "[n = 133]:\tloss = 1.575  exp loss = 1.636  adjusted loss = 1.636  adv prob = 0.250000   acc = 0.068\n",
      "[n = 133]:\tloss = 0.403  exp loss = 0.432  adjusted loss = 0.432  adv prob = 0.250000   acc = 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:18<00:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average incurred loss: 0.509  \n",
      "Average sample loss: 0.506  \n",
      "Average acc: 0.716  \n",
      "[n = 467]:\tloss = 0.096  exp loss = 0.095  adjusted loss = 0.095  adv prob = 0.250000   acc = 0.996\n",
      "[n = 466]:\tloss = 0.649  exp loss = 0.613  adjusted loss = 0.613  adv prob = 0.250000   acc = 0.577\n",
      "[n = 133]:\tloss = 1.575  exp loss = 1.636  adjusted loss = 1.636  adv prob = 0.250000   acc = 0.068\n",
      "[n = 133]:\tloss = 0.403  exp loss = 0.432  adjusted loss = 0.432  adv prob = 0.250000   acc = 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001000\n",
      "Current validation accuracy: 0.7155963182449341\n",
      "Best model saved at epoch 0\n",
      "\n",
      "\n",
      "Epoch [1]:\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 49/75 [10:48<05:42, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average incurred loss: 0.178  \n",
      "Average sample loss: 0.178  \n",
      "Average acc: 0.947  \n",
      "[n = 2319]:\tloss = 0.076  exp loss = 0.062  adjusted loss = 0.062  adv prob = 0.250000   acc = 0.997\n",
      "[n = 124]:\tloss = 0.738  exp loss = 0.856  adjusted loss = 0.856  adv prob = 0.250000   acc = 0.548\n",
      "[n = 38]:\tloss = 1.724  exp loss = 1.777  adjusted loss = 1.777  adv prob = 0.250000   acc = 0.132\n",
      "[n = 719]:\tloss = 0.330  exp loss = 0.289  adjusted loss = 0.289  adv prob = 0.250000   acc = 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 71/75 [15:52<00:53, 13.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_csv_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_csv_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_csv_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_csv_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_csv_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrobust\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrobust_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_step_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_normalized_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_normalized_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneralization_adjustment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneralization_adjustment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreweight_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreweight_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautomatic_adjustment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautomatic_adjustment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m train_csv_logger\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     27\u001b[0m val_csv_logger\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Programming\\ML-Algos\\GroupDRO\\train.py:121\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, dataset, logger, train_csv_logger, val_csv_logger, test_csv_logger, robust, alpha, gamma, robust_step_size, use_normalized_loss, generalization_adjustment, lr, weight_decay, n_epochs, log_every, save_step, save_last, save_best, log_dir, reweight_groups, automatic_adjustment, btl, minimum_variational_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m logger\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m epoch)\n\u001b[0;32m    120\u001b[0m logger\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_computer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loss_computer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m logger\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    135\u001b[0m val_loss_computer \u001b[38;5;241m=\u001b[39m LossComputer(\n\u001b[0;32m    136\u001b[0m     criterion,\n\u001b[0;32m    137\u001b[0m     is_robust\u001b[38;5;241m=\u001b[39mrobust,\n\u001b[0;32m    138\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_data\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    139\u001b[0m     step_size\u001b[38;5;241m=\u001b[39mrobust_step_size,\n\u001b[0;32m    140\u001b[0m     alpha\u001b[38;5;241m=\u001b[39malpha)\n",
      "File \u001b[1;32md:\\Programming\\ML-Algos\\GroupDRO\\train.py:41\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(epoch, model, optimizer, loader, loss_computer, logger, csv_logger, weight_decay, is_training, log_every)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[0;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mloss_main\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training \u001b[38;5;129;01mand\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programming\\ML-Algos\\venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\ML-Algos\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\ML-Algos\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    dataset=data,\n",
    "    logger=logger,\n",
    "    train_csv_logger=train_csv_logger,\n",
    "    val_csv_logger=val_csv_logger,\n",
    "    test_csv_logger=test_csv_logger,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    robust=robust,\n",
    "    robust_step_size=robust_step_size,\n",
    "    use_normalized_loss=use_normalized_loss,\n",
    "    generalization_adjustment=generalization_adjustment,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    n_epochs=n_epochs,\n",
    "    log_every=log_every,\n",
    "    save_step=save_step,\n",
    "    save_last=save_last,\n",
    "    save_best=save_best,\n",
    "    log_dir=log_dir,\n",
    "    reweight_groups=reweight_groups,\n",
    "    automatic_adjustment=automatic_adjustment)\n",
    "\n",
    "train_csv_logger.close()\n",
    "val_csv_logger.close()\n",
    "test_csv_logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
