{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAobOQBDYxOuTX9Xa67iAozyXwluvvsQhg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/waterbird/metadata.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDBIRD = 0\n",
    "WATERBIRD = 1\n",
    "LAND = 0\n",
    "WATER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 0]\n",
    "val_df = df[df['split'] == 1]\n",
    "test_df = df[df['split'] == 2]\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "val_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "\n",
    "len(train_df), len(test_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, 400\n",
    "test_df = test_df.iloc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\",\n",
    "                              system_instruction=\"Are those waterbirds or a landbirds? Just give birdtype for each image sperated by comma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = []\n",
    "batch = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    img_filename = row['img_filename']\n",
    "    img_label = int(row['y'])\n",
    "    place = int(row['place'])\n",
    "    \n",
    "    \n",
    "    batch.append({\n",
    "        'img_filename': img_filename,\n",
    "        'img_label': img_label,\n",
    "        'place': place\n",
    "    })\n",
    "\n",
    "    if (idx + 1) % 5 == 0:\n",
    "        images = []\n",
    "        for data in batch:\n",
    "            img = Image.open(os.path.join(\"../datasets/waterbird\", data['img_filename']))\n",
    "            images.append(img)\n",
    "\n",
    "        response = model.generate_content(images)\n",
    "        response = response.text.strip()\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        for prediction, entry in zip(response.split(\",\"), batch):\n",
    "            inference_data.append({\n",
    "                'img_filename': entry['img_filename'],\n",
    "                'img_label': entry['img_label'],\n",
    "                'place': entry['place'],\n",
    "                'predicted': prediction\n",
    "            })\n",
    "\n",
    "        batch = []\n",
    "        # break\n",
    "        \n",
    "        delay = 10\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./inference_data_{start}_{end - 1}.npy', inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.paligemma.modeling_paligemma import PaliGemmaPreTrainedModel, PaliGemmaMultiModalProjector, PaliGemmaCausalLMOutputWithPast\n",
    "from transformers import PaliGemmaConfig, SiglipVisionConfig, GemmaConfig\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from typing import Optional, Union, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PaliGemmaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_config = SiglipVisionConfig()\n",
    "text_config = GemmaConfig()\n",
    "configuration = PaliGemmaConfig(vision_config, text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel, GenerationMixin):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__(config)\n",
    "        # self.vision_tower = model.vision_tower\n",
    "        self.vision_tower = AutoModel.from_config(config=config.vision_config)\n",
    "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
    "        self.vocab_size = config.text_config.vocab_size\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if pixel_values is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        is_training = token_type_ids is not None and labels is not None\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length(\n",
    "            ) if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(\n",
    "                0) + 1  # Paligemma positions are 1-indexed\n",
    "\n",
    "        # Merge text and images\n",
    "        if pixel_values is not None:\n",
    "            image_outputs = self.vision_tower(\n",
    "                pixel_values.to(inputs_embeds.dtype))\n",
    "            selected_image_feature = image_outputs.last_hidden_state\n",
    "            image_features = self.multi_modal_projector(selected_image_feature)\n",
    "            image_features = image_features / (self.config.hidden_size**0.5)\n",
    "\n",
    "            special_image_mask = (\n",
    "                input_ids == self.config.image_token_index).unsqueeze(-1)\n",
    "            special_image_mask = special_image_mask.expand_as(\n",
    "                inputs_embeds).to(inputs_embeds.device)\n",
    "            if inputs_embeds[special_image_mask].numel() != image_features.numel():\n",
    "                image_tokens_in_text = torch.sum(\n",
    "                    input_ids == self.config.image_token_index)\n",
    "                raise ValueError(\n",
    "                    f\"Number of images does not match number of special image tokens in the input text. \"\n",
    "                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n",
    "                    \"tokens from image embeddings.\"\n",
    "                )\n",
    "            image_features = image_features.to(\n",
    "                inputs_embeds.device, inputs_embeds.dtype)\n",
    "            inputs_embeds = inputs_embeds.masked_scatter(\n",
    "                special_image_mask, image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomPaliGemmaForConditionalGeneration(configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
