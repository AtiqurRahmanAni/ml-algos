{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoImageProcessor, Swinv2ForImageClassification\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['HF_HOME'] = os.getcwd()\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.getcwd()\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "os.environ['HIP_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterbirdDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, image_processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.image_processor = image_processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.df.iloc[idx]['img_filename']\n",
    "        img = Image.open(os.path.join(self.root_dir, img_filename)).convert(\"RGB\")\n",
    "        \n",
    "        inputs = self.image_processor(img, return_tensors=\"pt\")\n",
    "        \n",
    "        label = self.df.iloc[idx]['y']\n",
    "        place = self.df.iloc[idx]['place']\n",
    "            \n",
    "        return img_filename, inputs.pixel_values, label, place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-base-patch4-window16-256 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/swinv2-base-patch4-window16-256\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = Swinv2ForImageClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/media/atiqur/Extra/Download/Waterbird/Swin-base/200_sample_1/swin_base_patch4_window16_256_augmented_200_sample_1.pth\", weights_only=True, map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/waterbird/metadata.csv')\n",
    "test_df = df[df['split'] == 2].reset_index(inplace=False)\n",
    "\n",
    "root_dir = './datasets/waterbird'\n",
    "test_set = WaterbirdDataset(df=test_df, root_dir=root_dir, image_processor=image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers  = [model.swinv2.layernorm]\n",
    "\n",
    "def reshape_transform(tensor, height=8, width=8):\n",
    "    result = tensor.reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "cam = GradCAM(model=HuggingfaceToTensorModelWrapper(model=model), target_layers=target_layers, reshape_transform=reshape_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ff253f849148cd9d0d36903095a08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = {\n",
    "    'img_filename': [],\n",
    "    'cam_filename': [],\n",
    "    'y': [],\n",
    "    'place': [],\n",
    "    'prediction': []\n",
    "}\n",
    "\n",
    "saving_dir = '/media/atiqur/Extra/Download/Waterbird/Swin-base/Grad-Cam'\n",
    "\n",
    "for i in tqdm(range(test_set.__len__())):\n",
    "    torch.cuda.empty_cache()\n",
    "    img_filename, inp, label, place = test_set.__getitem__(i)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(inp.to(DEVICE)).logits\n",
    "        pred = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "    inp = inp.to(DEVICE)\n",
    "    target = [ClassifierOutputTarget(label)]\n",
    "    \n",
    "    rgb_img = Image.open(os.path.join(root_dir, img_filename)).convert('RGB').resize((256, 256))\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "    cam_out = cam(input_tensor=inp, targets=target)\n",
    "    cam_out = cam_out[0, :]\n",
    "    visualization = show_cam_on_image(rgb_img, cam_out, use_rgb=True)\n",
    "\n",
    "    cam_filename = f'{i}_{img_filename.split('/')[-1]}'\n",
    "    metadata['img_filename'].append(img_filename)\n",
    "    metadata['cam_filename'].append(cam_filename)\n",
    "    metadata['y'].append(label)\n",
    "    metadata['place'].append(place)\n",
    "    metadata['prediction'].append(pred)\n",
    "\n",
    "    img = Image.fromarray(visualization)\n",
    "    img.save(os.path.join(saving_dir, cam_filename))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df.to_csv(os.path.join(saving_dir, 'metadata.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
